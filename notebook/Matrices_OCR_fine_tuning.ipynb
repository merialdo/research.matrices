{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Matrices_OCR_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP-v0E_S-mQP"
      },
      "source": [
        "# Matrices OCR training\n",
        "\n",
        "This tutorial shows how you can train the OCR module of the [Matrices repository](https://github.com/merialdo/research.matrices) in your Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMty1YwuWHpN"
      },
      "source": [
        "## 1 Disclaimer\n",
        "\n",
        "This colab loads and saves data from/into your Google Drive main folder.\n",
        "\n",
        "Please make sure that you have your dataset in your Google Drive main folder (in HDF5 format).\n",
        "\n",
        "When this Colab will ask your permission to access your Google Drive data, answer \"consent\".\n",
        "\n",
        "The training output (e.g., summary files and checkpoints of the training process) and any evaluation result files will be saved in a new folder created on your Google Drive main folder."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Parameters\n",
        "\n",
        "In this Python cell we define the main parameters of the training process."
      ],
      "metadata": {
        "id": "PwyWv_rUf7BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n"
      ],
      "metadata": {
        "id": "d91pP96JbLXv",
        "outputId": "38e4a794-f2f4-4804-fdda-bfd03a8a9598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 10.8 GB  |     Proc size: 5.2 GB\n",
            "GPU RAM Free: 357MB | Used: 11084MB | Util  97% | Total     11441MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string \n",
        "\n",
        "base_model = \"multilingual_model.hdf5\"  # name and path of the base model to fine-tune\n",
        "target_model_name = \"biagini_ft\"        # name of the model to create\n",
        "dataset_name = \"biagini.hdf5\"           # complete name of the dataset\n",
        "\n",
        "epochs = 100                       # number of training epochs\n",
        "batch_size = 16                    # number of samples per mini-batch\n",
        "#learning_rate=0.001\n",
        "learning_rate=0.0005               # learning rate of the training process\n",
        "\n",
        "input_size = (1024, 128, 1)       # input size of the images to transcribe \n",
        "#input_size = (640, 64, 1)\n",
        "\n",
        "max_text_length = 128             # max number of characters for each transcribed line\n",
        "#max_text_length = 180\n",
        "\n",
        "validation_interval = 1            # number of training epochs between validations\n",
        "\n",
        "charset_base = string.printable[:95]#+\"€\"     # alphabet of available characters"
      ],
      "metadata": {
        "id": "Lp2E-aoef5V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jydsAcWgWVth"
      },
      "source": [
        "## 3 Google Colab Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk3e7YJiXzSl"
      },
      "source": [
        "### 3.1 TensorFlow 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7twXyNGXtbJ"
      },
      "source": [
        "Make sure the jupyter notebook is using GPU mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHw4tODULT1Z",
        "outputId": "006d60cc-2f31-48bc-9319-dc2c5687cd2b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 22 10:50:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    74W / 149W |  11084MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJECz8H8XVCY"
      },
      "source": [
        "Install TensorFlow 2.x., and verify it has been installed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMg-B5PH9h3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00821ce9-7a5b-4c64-dd09-beb99aad0055"
      },
      "source": [
        "#!pip install -q tensorflow-gpu\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "\n",
        "print(f\"Found GPU at: {device_name}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyMv5wyDXxqc"
      },
      "source": [
        "### 3.2 Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gj6qwoX9W3"
      },
      "source": [
        "Mount your Google Drive partition; after this step, you should be able to see the list of your Google Drive files in the project folder, under path /content/drive.\n",
        "\n",
        "Note: the project folder is temporary storage exclusive to this Colab notebook, and it is now only partially connected to your Google Drive. So: \n",
        "- interacting with files in /content/drive actively changes the content of your Google Drive\n",
        "- but any changes to other paths in the project folder will likely be erased reloading the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRH17YGlNR1X",
        "outputId": "1c81dfb1-cee1-4a57-f9e9-47733ffcf907"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnR-wKP88fng"
      },
      "source": [
        "### 3.3 Clone the Matrices Repository from GitHub\n",
        "\n",
        "Clone the research.matrices repository into the project folder in path /content/matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecRUt0Sz_iPT",
        "outputId": "ff19e12b-3b59-49fd-e130-2b7662750a1e"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "if os.path.isdir(\"matrices\"):\n",
        "  shutil.rmtree(\"matrices\")\n",
        "os.mkdir(\"matrices\")\n",
        "\n",
        "!git clone https://github.com/merialdo/research.matrices matrices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'matrices'...\n",
            "remote: Enumerating objects: 472, done.\u001b[K\n",
            "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 472 (delta 119), reused 196 (delta 93), pack-reused 231\u001b[K\n",
            "Receiving objects: 100% (472/472), 10.22 MiB | 22.16 MiB/s, done.\n",
            "Resolving deltas: 100% (190/190), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the backend requirements for Matrices:"
      ],
      "metadata": {
        "id": "qlglKVBKKjit"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZlBqQk9s-XH",
        "outputId": "99664c3a-d886-476f-ff5d-3f5da8dd668e"
      },
      "source": [
        "!pip install -r /content/matrices/backend/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 1)) (0.1.12)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 4)) (3.0.10)\n",
            "Requirement already satisfied: flask-mongoengine in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: flask-RESTful in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 6)) (0.3.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: mongoengine in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 8)) (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 9)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 10)) (4.1.2.30)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 11)) (1.3.0.post2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 12)) (2.23.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 13)) (1.8.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 14)) (2.7.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 15)) (2.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/matrices/backend/requirements.txt (line 16)) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (0.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (0.18.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations->-r /content/matrices/backend/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/matrices/backend/requirements.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/matrices/backend/requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/matrices/backend/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r /content/matrices/backend/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->-r /content/matrices/backend/requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: WTForms[email]>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from flask-mongoengine->-r /content/matrices/backend/requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: Flask-WTF>=0.14.3 in /usr/local/lib/python3.7/dist-packages (from flask-mongoengine->-r /content/matrices/backend/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: pymongo<4.0,>=3.4 in /usr/local/lib/python3.7/dist-packages (from mongoengine->-r /content/matrices/backend/requirements.txt (line 8)) (3.12.3)\n",
            "Requirement already satisfied: email-validator in /usr/local/lib/python3.7/dist-packages (from WTForms[email]>=2.3.1->flask-mongoengine->-r /content/matrices/backend/requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.7/dist-packages (from flask-RESTful->-r /content/matrices/backend/requirements.txt (line 6)) (9.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-RESTful->-r /content/matrices/backend/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->-r /content/matrices/backend/requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/matrices/backend/requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/matrices/backend/requirements.txt (line 12)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/matrices/backend/requirements.txt (line 12)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r /content/matrices/backend/requirements.txt (line 12)) (2.10)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (1.43.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/matrices/backend/requirements.txt (line 14)) (3.1.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (1.6.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (12.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (1.13.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (2.7.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (0.23.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/matrices/backend/requirements.txt (line 15)) (2.0)\n",
            "Requirement already satisfied: dnspython>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from email-validator->WTForms[email]>=2.3.1->flask-mongoengine->-r /content/matrices/backend/requirements.txt (line 5)) (2.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a \"data\" folder in the matrices project and copy the dataset from Google Drive to that folder"
      ],
      "metadata": {
        "id": "Ynyzx0bOPOPv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR9WqzvL3k5_"
      },
      "source": [
        "matrices_data_folder = \"/content/matrices/data\"\n",
        "\n",
        "# define paths\n",
        "dataset_path = os.path.join(matrices_data_folder, dataset_name)\n",
        "output_path = os.path.join(\"/content/drive/MyDrive\", target_model_name)\n",
        "checkpoint_path = os.path.join(output_path, \"checkpoint_model.{epoch:02d}.hdf5\")\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(matrices_data_folder):\n",
        "  shutil.rmtree(matrices_data_folder)\n",
        "os.mkdir(matrices_data_folder)\n",
        "\n",
        "shutil.copyfile(os.path.join(\"/content/drive/MyDrive\", dataset_name), os.path.join(matrices_data_folder, dataset_name))"
      ],
      "metadata": {
        "id": "wIPNu3hrhw4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb3232d1-64b7-43bc-9721-ffcff73a209b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/matrices/data/biagini.hdf5'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fj7fSngY1IX"
      },
      "source": [
        "## 4 Run the Training Process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move to the Matrices folder and perform the actual training process.\n",
        "At constant intervals defined in the validation_interval parameters, a validation run will be executed on the validation set; if the model loss on the validation set has improved since the last validation run, the checkpoint of the current epoch will be saved on Google Drive.\n",
        "\n",
        "The training process keeps track of which epoch has been the \"best\" one in terms of validation loss."
      ],
      "metadata": {
        "id": "XpgjrL8Yi3Xi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CULMSRemASel",
        "outputId": "248b8ad9-1193-4904-c94b-d4f6568087b3"
      },
      "source": [
        "%cd '/content/matrices/'\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/matrices/backend/ocr_service/\")\n",
        "\n",
        "from backend.ocr_service.model import HTRModel\n",
        "import backend.ocr_service.evaluation as evaluation\n",
        "from backend.ocr_service.dataset import HDF5Dataset\n",
        "\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "\n",
        "try:\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
        "    logging.disable(logging.WARNING)\n",
        "except AttributeError:\n",
        "    pass\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "print(\"Dataset: \", dataset_path)\n",
        "print(\"Output model folder: \", output_path)\n",
        "\n",
        "# load the dataset to use in training, validation and testing\n",
        "dataset = HDF5Dataset(source_path=dataset_path,\n",
        "                      batch_size=batch_size,\n",
        "                      charset=charset_base,\n",
        "                      max_text_length=max_text_length)\n",
        "print(f\"Train images:      {dataset.training_set_size}\")\n",
        "print(f\"Validation images: {dataset.valid_set_size }\")\n",
        "print(f\"Test images:       {dataset.test_set_size}\")\n",
        "\n",
        "# create and compile HTRModel\n",
        "htr_model = HTRModel(input_size=input_size,\n",
        "                     vocabulary_size=dataset.tokenizer.vocab_size,\n",
        "                     beam_width=10,\n",
        "                     stop_tolerance=25,\n",
        "                     reduce_tolerance=20)\n",
        "\n",
        "htr_model.compile(learning_rate=learning_rate)\n",
        "htr_model.summary(output_path, \"summary.txt\")\n",
        "\n",
        "# load model\n",
        "resumed_model = os.path.join(\"/content/drive/MyDrive\", base_model) \n",
        "htr_model.load_checkpoint(target=resumed_model)\n",
        "callbacks = htr_model.get_callbacks(logdir=output_path,\n",
        "                                    checkpoint=checkpoint_path,\n",
        "                                    verbose=1)\n",
        "\n",
        "# to calculate total and average time per epoch\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "\n",
        "htr_model_history = htr_model.fit(x=dataset.training_data_generator,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=dataset.valid_data_generator,\n",
        "                                  validation_freq=validation_interval,\n",
        "                                  callbacks=callbacks,\n",
        "                                  verbose=1)\n",
        "\n",
        "training_time = datetime.datetime.now() - start_time\n",
        "\n",
        "loss = htr_model_history.history['loss']\n",
        "val_loss = htr_model_history.history['val_loss']\n",
        "\n",
        "min_val_loss = min(val_loss)\n",
        "min_val_loss_i = val_loss.index(min_val_loss)\n",
        "\n",
        "avg_epoch_time = (training_time / len(loss))\n",
        "best_epoch = (min_val_loss_i + 1) * validation_interval\n",
        "\n",
        "t_corpus = \"\\n\".join([\n",
        "    f\"Total validation images: {dataset.valid_data_generator}\",\n",
        "    f\"Batch Size:              {dataset.training_data_generator.batch_size}\\n\",\n",
        "    f\"Total Training Time:     {training_time}\",\n",
        "    f\"Time per epoch:          {avg_epoch_time}\",\n",
        "    f\"Total epochs:            {len(loss)}\",\n",
        "    f\"Best epoch:              {best_epoch}\",\n",
        "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
        "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
        "])\n",
        "\n",
        "\n",
        "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
        "    lg.write(t_corpus)\n",
        "    print(t_corpus)\n",
        "  \n",
        "print(\"The best epoch for validation loss has been Epoch \" + str(best_epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/matrices\n",
            "Dataset:  /content/matrices/data/biagini.hdf5\n",
            "Output model folder:  /content/drive/MyDrive/biagini_ft\n",
            "Train images:      1179\n",
            "Validation images: 460\n",
            "Test images:       371\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 1024, 128, 1)]    0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 1024, 64, 32)      320       \n",
            "                                                                 \n",
            " p_re_lu_18 (PReLU)          (None, 1024, 64, 32)      32        \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 1024, 64, 32)     224       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " full_gated_conv2d_15 (FullG  (None, 1024, 64, 32)     18496     \n",
            " atedConv2D)                                                     \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 1024, 64, 64)      18496     \n",
            "                                                                 \n",
            " p_re_lu_19 (PReLU)          (None, 1024, 64, 64)      64        \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 1024, 64, 64)     448       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " full_gated_conv2d_16 (FullG  (None, 1024, 64, 64)     73856     \n",
            " atedConv2D)                                                     \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 512, 16, 128)      65664     \n",
            "                                                                 \n",
            " p_re_lu_20 (PReLU)          (None, 512, 16, 128)      128       \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 512, 16, 128)     896       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " full_gated_conv2d_17 (FullG  (None, 512, 16, 128)     295168    \n",
            " atedConv2D)                                                     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 512, 16, 128)      0         \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 512, 16, 256)      295168    \n",
            "                                                                 \n",
            " p_re_lu_21 (PReLU)          (None, 512, 16, 256)      256       \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 512, 16, 256)     1792      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " full_gated_conv2d_18 (FullG  (None, 512, 16, 256)     1180160   \n",
            " atedConv2D)                                                     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 512, 16, 256)      0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 256, 4, 256)       524544    \n",
            "                                                                 \n",
            " p_re_lu_22 (PReLU)          (None, 256, 4, 256)       256       \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 256, 4, 256)      1792      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " full_gated_conv2d_19 (FullG  (None, 256, 4, 256)      1180160   \n",
            " atedConv2D)                                                     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256, 4, 256)       0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 256, 4, 512)       1180160   \n",
            "                                                                 \n",
            " p_re_lu_23 (PReLU)          (None, 256, 4, 512)       512       \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 256, 4, 512)      3584      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 256, 2, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 512, 512)          0         \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 512, 256)         656384    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512, 256)          65792     \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 512, 256)         394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512, 98)           25186     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,983,778\n",
            "Trainable params: 5,977,538\n",
            "Non-trainable params: 6,240\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "99/99 [==============================] - ETA: 0s - loss: 6.2347"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f7b9919e55ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                                   \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                   verbose=1)\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/matrices/backend/ocr_service/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m                              \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                              \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                              use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  failed to allocate memory\n\t [[node model_3/p_re_lu_18/Relu\n (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:4867)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[ctc_loss_lambda_func/Cast_1/_78]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  failed to allocate memory\n\t [[node model_3/p_re_lu_18/Relu\n (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:4867)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_78764]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_3/p_re_lu_18/Relu:\nIn[0] model_3/conv2d_18/BiasAdd (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py:265)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-34-f7b9919e55ef>\", line 60, in <module>\n>>>     verbose=1)\n>>> \n>>>   File \"/content/matrices/backend/ocr_service/model.py\", line 240, in fit\n>>>     use_multiprocessing=use_multiprocessing, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1263, in fit\n>>>     _use_cached_eval_dataset=True)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1537, in evaluate\n>>>     tmp_logs = self.test_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1366, in test_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1356, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1349, in run_step\n>>>     outputs = model.test_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1303, in test_step\n>>>     y_pred = self(x, training=False)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/layers/advanced_activations.py\", line 164, in call\n>>>     pos = backend.relu(inputs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4867, in relu\n>>>     x = tf.nn.relu(x)\n>>> \n\nInput Source operations connected to node model_3/p_re_lu_18/Relu:\nIn[0] model_3/conv2d_18/BiasAdd (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py:265)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-34-f7b9919e55ef>\", line 60, in <module>\n>>>     verbose=1)\n>>> \n>>>   File \"/content/matrices/backend/ocr_service/model.py\", line 240, in fit\n>>>     use_multiprocessing=use_multiprocessing, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1263, in fit\n>>>     _use_cached_eval_dataset=True)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1537, in evaluate\n>>>     tmp_logs = self.test_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1366, in test_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1356, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1349, in run_step\n>>>     outputs = model.test_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1303, in test_step\n>>>     y_pred = self(x, training=False)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/layers/advanced_activations.py\", line 164, in call\n>>>     pos = backend.relu(inputs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4867, in relu\n>>>     x = tf.nn.relu(x)\n>>> \n\nFunction call stack:\ntest_function -> test_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run evaluation on the test set, and extract\n",
        "- global metrics:\n",
        " - Character Error Rate\n",
        " - Word Error Rate\n",
        " - Sequence Error Rate\n",
        "\n",
        "- A visual representation of line image, original text and obtained transcription for each sample in the test set. "
      ],
      "metadata": {
        "id": "wg-Wdcclj69-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# load the checkpoint of the best epoch\n",
        "best_checkpoint_path = os.path.join(output_path, \"checkpoint_model.\" + str(best_epoch) + \".hdf5\")\n",
        "htr_model.load_checkpoint(target=best_checkpoint_path)\n",
        "\n",
        "# predict() function will return the predicts with the probabilities\n",
        "predicts, prob = htr_model.predict(x=dataset.test_data_generator,\n",
        "                                   steps=1,\n",
        "                                   ctc_decode=True,\n",
        "                                   verbose=1,\n",
        "                                   use_multiprocessing=False)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "predicts = tf.sparse.to_dense(predicts[0]).numpy()\n",
        "prob = prob.numpy()\n",
        "\n",
        "# decode to string\n",
        "predicts = [dataset.tokenizer.decode(x) for x in predicts]\n",
        "ground_truth = [x.decode() for x in dataset.test_data_generator.labels]\n",
        "\n",
        "# mount predict corpus file\n",
        "with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n",
        "    for pd, gt in zip(predicts, ground_truth):\n",
        "        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n",
        "print(len(predicts), len(ground_truth), len(prob))\n",
        "\n",
        "evaluate = evaluation.ocr_metrics(predicts,\n",
        "                                  ground_truth,\n",
        "                                  norm_accentuation=True,\n",
        "                                  norm_punctuation=True)\n",
        "\n",
        "e_corpus = \"\\n\".join([\n",
        "    f\"Metrics:\",\n",
        "    f\"Character Error Rate: {evaluate[0]:.8f}\",\n",
        "    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n",
        "    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
        "    lg.write(e_corpus)\n",
        "    print(e_corpus)\n",
        "\n",
        "from backend.ocr_service.image_processing import adjust_to_see\n",
        "\n",
        "for j, item in enumerate(dataset.test_data_generator.samples):\n",
        "    print(\"=\" * 256, \"\\n\")\n",
        "    cv2_imshow(adjust_to_see(item))\n",
        "    print(ground_truth[j])\n",
        "    print(predicts[j])\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "Y-jEDo4jJ1RR",
        "outputId": "dcb6eba0-c987-45ad-b7af-1ada043eab08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e680cb39b742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load the checkpoint of the best epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint_model.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhtr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_epoch' is not defined"
          ]
        }
      ]
    }
  ]
}